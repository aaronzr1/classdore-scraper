name: Scrape listings (-l) and push (data-pipeline)

on:
  schedule:
    - cron: "30 8 * * *"  # 3:30 AM CT â†’ 8:30 UTC
  workflow_dispatch:

jobs:
  run-l:
    runs-on: ubuntu-latest

    steps:
      - name: Checkout repo
        uses: actions/checkout@v4
        with:
          token: ${{ secrets.GH_PAT }} # personal access token with repo write access

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"

      - name: Install deps
        run: pip install -r requirements.txt

      - name: Set Git identity
        run: |
          git config user.name "GitHub Actions"
          git config user.email "actions@github.com"

      - name: Merge main into data-pipeline
        run: |
          git fetch origin
          git checkout -B data-pipeline origin/data-pipeline
          git merge origin/main --allow-unrelated-histories --no-edit || echo "No changes to merge"
          git push origin data-pipeline

      - name: Check for UPLOAD_ONLY marker
        id: check_upload_only
        run: |
          if [ -f "UPLOAD_ONLY" ]; then
            echo "UPLOAD_ONLY exists, skipping -l job."
            echo "skip=true" >> $GITHUB_OUTPUT
          else
            echo "skip=false" >> $GITHUB_OUTPUT
          fi

      - name: Scrape listings (-l)
        if: steps.check_upload_only.outputs.skip == 'false'
        run: python scraper.py -l

      - name: Commit and push (data-pipeline branch)
        if: steps.check_upload_only.outputs.skip == 'false'
        run: |
          git add data/course_listings.json
          git commit -m "Automated -l output for $(date -u +"%Y-%m-%d %H:%M:%S UTC")" || echo "No changes"
          git push origin data-pipeline