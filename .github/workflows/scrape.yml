name: Scrape data, upload to redis, and push (data-pipeline)

on:
  schedule:
    - cron: "30 8 * * *"  # 3:30 AM CT â†’ 8:30 UTC (full scrape)
    - cron: "17 * * * *"  # Every hour at minute 17 (delta scrape)
  workflow_dispatch:
    inputs:
      skip_scrape:
        description: 'Skip scraping step'
        type: boolean
        default: false
      details_only:
        description: 'Only scrape details (skip listings generation)'
        type: boolean
        default: false
      skip_upload:
        description: 'Skip Redis upload step'
        type: boolean
        default: false
      skip_commit:
        description: 'Skip commit and push step'
        type: boolean
        default: false
      delete_data_json:
        description: 'Debug: Delete data/data.json before running'
        type: boolean
        default: false

jobs:
  scrape-data:
    runs-on: ubuntu-latest

    env:
      REDIS_URL: ${{ secrets.REDIS_URL }}

    steps:
      - name: Checkout repo
        uses: actions/checkout@v4
        with:
          token: ${{ secrets.GH_PAT }} # personal access token with repo write access

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.12"

      - name: Install deps
        run: pip install -r requirements.txt

      - name: Set Git identity
        run: |
          git config user.name "GitHub Actions"
          git config user.email "actions@github.com"

      - name: Merge main into data-pipeline
        run: |
          git fetch origin
          git checkout -B data-pipeline origin/data-pipeline

          # Pull the latest from main, but skip overwriting data/
          git checkout origin/main -- . ':!data/'

          # Commit only if there are changes
          if ! git diff --cached --quiet; then
            git commit -am "Sync main into data-pipeline (preserving data/)"
            git push origin data-pipeline
          else
            echo "No changes to merge"
          fi

      - name: Delete data.json (debug)
        if: inputs.delete_data_json
        run: |
          if [ -f "data/data.json" ]; then
            rm data/data.json
            echo "Deleted data/data.json"
          else
            echo "data/data.json does not exist"
          fi

      - name: Check for UPLOAD_ONLY marker
        id: check_upload_only
        run: |
          if [ -f "UPLOAD_ONLY" ]; then
            echo "UPLOAD_ONLY exists, skipping -l job."
            echo "skip=true" >> $GITHUB_OUTPUT
          else
            echo "skip=false" >> $GITHUB_OUTPUT
          fi

      - name: Determine scrape type and add random delay
        id: scrape_type
        run: |
          HOUR=$(date -u +%H)
          MINUTE=$(date -u +%M)
          if [ "$HOUR" = "08" ] && [ "$MINUTE" = "30" ]; then
            echo "Running full scrape (scheduled 3:30 AM CT)"
            echo "is_delta=false" >> $GITHUB_OUTPUT
          else
            echo "Running delta scrape (hourly)"
            echo "is_delta=true" >> $GITHUB_OUTPUT
            # Random delay between 0-20 minutes (0-1200 seconds)
            DELAY=$((RANDOM % 1201))
            echo "Adding random delay of $DELAY seconds (~$((DELAY / 60)) minutes)"
            sleep $DELAY
          fi

      - name: Run scraper
        if: steps.check_upload_only.outputs.skip == 'false' && !inputs.skip_scrape
        run: |
          if [ "${{ inputs.details_only }}" = "true" ]; then
            echo "Running details-only scrape (using existing course_listings.json)"
            python scraper.py -d
          elif [ "${{ steps.scrape_type.outputs.is_delta }}" = "true" ]; then
            echo "Running delta scrape (using existing course_listings.json)"
            python scraper.py -d
          else
            echo "Running full scrape (listings + details)"
            python scraper.py
          fi

      - name: Check if data.json has changes
        id: check_data_changes
        run: |
          if git diff --quiet -- data/data.json; then
            echo "No changes detected in data/data.json"
            echo "has_changes=false" >> $GITHUB_OUTPUT
          else
            echo "Changes detected in data/data.json"
            echo "has_changes=true" >> $GITHUB_OUTPUT
          fi

      - name: Upload to Redis
        if: steps.check_data_changes.outputs.has_changes == 'true' && !inputs.skip_upload
        run: python upload_to_redis.py data/data.json

      - name: Commit and push (data-pipeline branch)
        if: ${{ !inputs.skip_commit }}
        run: |
          git add data/course_listings.json
          git add data/data.json
          git commit -m "Automated output for $(date -u +"%Y-%m-%d %H:%M:%S UTC")" || echo "No changes"
          git push origin data-pipeline